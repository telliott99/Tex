\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\usepackage{amsmath}

\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{parskip}

\graphicspath{{/Users/telliott_admin/Dropbox/Tex/png/}}

\title{Matrix proofs}
%\author{The Author}
\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle
%\subsection{}
\Large
\noindent
\subsection*{associativity of matrix multiplication}
This is the first important property to prove.  If $A$,$B$ and $C$ are three matrices with the correct shape to be multiplied together as $ABC$, then 
\[ ABC = (AB)C = A(BC) \]

Example:
\[
\begin{bmatrix}
a & b \\
c & d 
\end{bmatrix}
\begin{bmatrix}
e & f \\
g & h 
\end{bmatrix}
\begin{bmatrix}
i & j\\
k & l 
\end{bmatrix}
=
\begin{bmatrix}
ae + bg & af + bh \\
ce + dg & cf + dh 
\end{bmatrix}
\begin{bmatrix}
i & j\\
k & l 
\end{bmatrix}
=
\]
The entry at the upper left will be 
\[ (ae + bg, af + bh) \cdot (i,k) = aei + bgi + afk + bhk \]
Alternatively
\[
\begin{bmatrix}
a & b \\
c & d 
\end{bmatrix}
\begin{bmatrix}
e & f \\
g & h 
\end{bmatrix}
\begin{bmatrix}
i & j\\
k & l 
\end{bmatrix}
=
\begin{bmatrix}
a & b\\
c & d
\end{bmatrix}
\begin{bmatrix}
ei + fk & ej + fl \\
gi + hk & gj + hl 
\end{bmatrix}
=
\]
The entry at the upper left will be 
\[ (a,b) \cdot (ei + fk, gi + hk) = aei + afk + bgi + bhk \]
Thus we see that associativity of matrix multiplication depends on associativity of the dot product.  Using parentheses for the first dot product and angle brackets for the second, multiplying $AB$ first gives
\[ \langle (a,b) \cdot (e,g), (a,b) \cdot (f,h)\rangle \cdot \langle i,k \rangle \]
\[ = \langle ae + bg, af + bh \rangle \cdot \langle i, k \rangle \]
\[ = aei + bgi + afk + bhk \]
while multiplying $BC$ first gives 
\[ \langle a,b \rangle \cdot \langle (e,f) \cdot (i,k), (g,h) \cdot (i,k) \rangle  \]
\[ \langle a,b \rangle \cdot \langle ei + fk, gi + hk \rangle \]
\[ = aei + afk + bgi  + bhk \]

\subsection*{inverse of AB}
Suppose we have two square, invertible matrices, $A$ and $B$, that are of the same size and can be multiplied together to give the product $AB$. Then
\[ (AB)^{-1} = B^{-1} A^{-1} \]
Let $C = B^{-1} A^{-1}$.  We need to show that
\[ C(AB) = (AB)C = I \]
Substitute
\[ B^{-1} A^{-1}(AB) = (AB)B^{-1} A^{-1} = I \]
Having proved associativity (above), this is trivial.  For multiplication with the inverse on the left side.
\[ B^{-1} A^{-1}(AB) = B^{-1} (A^{-1}A) B = B^{-1} I B = B^{-1} B = I \]
Multiplication on the right side is similar.
\[ (AB)B^{-1} A^{-1} = A (B B^{-1}) A^{-1} = A I A^{-1} = A A^{-1} = I \]

\subsection*{transpose of A}
The transpose exchanges columns and rows.  If
\[ A = 
\begin{bmatrix}
a & b \\
c & d 
\end{bmatrix} 
\ \ \Rightarrow
\ \ 
A^T =
\begin{bmatrix}
a & c \\
b & d 
\end{bmatrix}
\]
\[ M = 
\begin{bmatrix}
a & b & c \\
d & e & f \\
g & h & i
\end{bmatrix} 
\ \ \Rightarrow
\ \ 
M^T =
\begin{bmatrix}
a & d & g \\
b & e & h \\
c & f & i
\end{bmatrix}
\]
Clearly, $(A^T)^T = A$ and $(M^T)^T = M$.  Because of the exchange, if we reverse the order of multiplication, we get the same terms, but in transposed positions:
\[ (AB)^T = B^T A^T \]
Consider the $2 \times 2$ case
\[ AB = 
\begin{bmatrix}
a & b \\
c & d 
\end{bmatrix} 
\begin{bmatrix}
e & f \\
g & h 
\end{bmatrix} 
=
\begin{bmatrix}
ae + bg & af + bh \\
ce + dg & cf + dh 
\end{bmatrix}
\]

\[ B^T A^T = 
\begin{bmatrix}
e & g \\
f & h 
\end{bmatrix} 
\begin{bmatrix}
a & c \\
b & d 
\end{bmatrix} 
=
\begin{bmatrix}
ae + bg & ce + dg \\
af + bh & cf + dh 
\end{bmatrix}
\]
We saw a formal proof of this property in the short write-up on properties of the determinant.

\subsection*{transpose and inverse}
Finally,
\[ (A^T)^{-1} = (A^{-1})^T \]

Example:
\[ A = 
\begin{bmatrix}
a & b \\
c & d 
\end{bmatrix} 
\]
Recalling the quick formula for inverse in the $2 \times 2$ case, the cofactor is the determinant, switch positions along the diagonal, and change signs off the diagonal:
\[ A^{-1} = 
\frac{1}{ad-bc}
\begin{bmatrix}
\ \ d & -b \\
-c & \ \ a 
\end{bmatrix} 
\]
\[ (A^{-1})^T = 
\frac{1}{ad-bc}
\begin{bmatrix}
\ \ d & -c \\
-b & \ \ a 
\end{bmatrix} 
\]
\[ A^T = 
\begin{bmatrix}
a & c \\
b & d 
\end{bmatrix} 
\]
\[ (A^T)^{-1} = 
\frac{1}{ad-bc}
\begin{bmatrix}
\ \ d & -c \\
-b & \ \ a 
\end{bmatrix} 
\]
Here is a quick proof:
\[ A A^{-1} = I \]
Take the transpose of both sides
\[ (A A^{-1})^T = I^T = I \]
by the property discussed in the previous section, the left-hand side is
\[ = (A^{-1})^T A^T \]
That is
\[ (A^{-1})^T A^T = I \]
Thus, by the definition of the inverse, $(A^{-1})^T$ is the inverse of $A^T$ for left-multiplication
\[ (A^T)^{-1} = (A^{-1})^T  \]
Reverse the order of $A$ and $A^{-1}$ to prove the same for right-multiplication.

\end{document}  